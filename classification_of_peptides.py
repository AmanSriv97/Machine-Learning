# -*- coding: utf-8 -*-
"""classification of peptides.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/115RbwvbeQEWQ4uKL1gqOo92TtXcF_ZB_
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

## Importing the required libraries.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split                       

import keras

# Takes train csv file input form the user.
print("Enter the train csv file location")
train_input= input()

# Takes test csv file input form the user.
print("Enter the test csv file location")
test_input= input()

##Reading the train csv using pandas
train = pd.read_csv(r"{}".format(train_input))

##Reading the test csv using pandas
test = pd.read_csv(r"{}".format(test_input))

train.head()

test.head()

# Preprocessing the text

codes = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',
         'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']

def create_dict(codes):
  char_dict = {}
  for index, val in enumerate(codes):
    char_dict[val] = index+1

  return char_dict

char_dict = create_dict(codes)


def integer_encoding(data):
    
  """- Encodes code sequence to integer values.
  - 20 common amino acids are taken into consideration
  and rest 4 are categorized as 0.
  """
  
  encode_list = []
  for row in data['Sequence'].values:
    row_encode = []
    for code in row:
      row_encode.append(char_dict.get(code, 0))
    encode_list.append(np.array(row_encode))
  
  return encode_list

train_encode = integer_encoding(train)

#Padding the sequences to equal length

from keras.preprocessing.sequence import pad_sequences

max_length = 30
train_pad = pad_sequences(train_encode, maxlen=max_length, padding='post', truncating='post')

Y = train.to_numpy()[:, 1]
Y = np.array([[1, 0] if i == 0 else [0, 1] for i in Y])

from tensorflow.keras.utils import to_categorical

# One hot encoding of sequences
train_ohe = to_categorical(train_pad)

from keras.regularizers import l2

#Model Bidirectional LSTM


x_input = keras.layers.Input(shape=(max_length,))
emb = keras.layers.Embedding(21, 128, input_length=max_length)(x_input)
bi_rnn = keras.layers.Bidirectional(keras.layers.CuDNNLSTM(64, kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)))(emb)
x = keras.layers.Dropout(0.3)(bi_rnn)

# softmax classifier
x_output = keras.layers.Dense(2, activation='softmax')(x)

model1 = keras.models.Model(inputs=x_input, outputs=x_output)
model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model1.summary()

model1.fit(train_pad, Y, epochs = 20)

#padding the test data
test_encode = integer_encoding(test) 

from keras.preprocessing.sequence import pad_sequences

max_length = 25
test_pad = pad_sequences(test_encode, maxlen=max_length, padding='post', truncating='post')

#model predictions

predictions = model1.predict(test_pad)

predictions = np.array([0 if i[0] >= 0.5 else 1 for i in predictions])

predictions

print("Enter Sample File address")
sample = pd.read_csv(r"{}".format(input()))

sample[sample.columns[1]] = predictions

sample.columns = ['ID', 'Label']

sample.to_csv("/content/drive/MyDrive/classification of peptides/result.csv", index=False)       ## Saving the result to csv









def residual_block(data, filters, d_rate):
  """
  _data: input
  _filters: convolution filters
  _d_rate: dilation rate
  """

  shortcut = data

  bn1 = BatchNormalization()(data)
  act1 = Activation('relu')(bn1)
  conv1 = Conv1D(filters, 1, dilation_rate=d_rate, padding='same', kernel_regularizer=l2(0.001))(act1)

  #bottleneck convolution
  bn2 = BatchNormalization()(conv1)
  act2 = Activation('relu')(bn2)
  conv2 = Conv1D(filters, 3, padding='same', kernel_regularizer=l2(0.001))(act2)

  #skip connection
  x = Add()([conv2, shortcut])

  return x

import tensorflow as tf
import numpy as np

from numpy import array
from keras.models import Sequential
from keras.layers import LSTM
from keras.layers import Dense

from keras.layers.normalization import BatchNormalization
from keras.layers import Activation
# input
x_input = keras.layers.Input(shape=(100, 21))

#initial conv
conv = keras.layers.Conv1D(128, 1, padding='same')(x_input) 

# per-residue representation
res1 = residual_block(conv, 128, 2)
res2 = residual_block(res1, 128, 3)

x = MaxPooling1D(3)(res2)
x = Dropout(0.5)(x)

# softmax classifier
x = Flatten()(x)
x_output = Dense(1000, activation='softmax', kernel_regularizer=l2(0.0001))(x)

model2 = Model(inputs=x_input, outputs=x_output)
model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

"""##Encoder Class

class Encoder:
  def __init__(self):
    self.A = "HRPK"
    self.B = "DEN"
    self.C = "CAL"
    self.D = "STG"
    self.E = "MIQV"
    self.F = "FYW"

  def ind(self, substr, n):
    res = 0
    for i in range(len(substr)):
      res *= 6
      res += ord(substr[i])-ord('A')
    return res
  
  def group_encode(self, prot):
    encoded = ""
    for i in prot:
      if i in self.A:
        encoded += "A"
      elif i in self.B:
        encoded += "B"
      elif i in self.C:                                                                             
        encoded += "C"
      elif i in self.D:
        encoded += "D"
      elif i in self.E:
        encoded += "E"
      else:
        encoded += "F"
    
    return encoded
  
  ##Defined function for doing n gram encoding. Here n is taken to be 2 to do bi-gram protein feature extraction.
  def n_gram_encode(self, prot, n):
    group_encoded_prot = self.group_encode(prot)
    encoded = [0]*(6**n)
    for i in range(len(group_encoded_prot)-n+1):
      encoded[self.ind(group_encoded_prot[i:i+n], n)] += 1
    
    return encoded
    """

from google.colab import drive
drive.mount('/content/drive')

"""## Encoding the Dataset
enc = Encoder()
X = train.to_numpy()[:, 1]
X_encoded = [enc.n_gram_encode(i, n_gram) for i in X]                         
Y = train.to_numpy()[:, 0]
Y = [[1, 0] if i == 0 else [0, 1] for i in Y]

## Balancing Dataset
for i in range(len(Y)):
  if Y[i][0] == 0:
    for j in range(16):                                                         
      Y.append(Y[i])
      X_encoded.append(X_encoded[i])

X_encoded = np.array(X_encoded).reshape(-1, 6**n_gram)
Y = np.array(Y).reshape(-1, 2)
"""

## Splitting Dataset using train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X_encoded, Y, test_size=0.30, random_state = 101)

##Preparing Keras model
model = keras.models.Sequential()

model.add(keras.layers.Dense(units = 72, activation='relu', input_shape=(6**n_gram,)))
model.add(keras.layers.Dense(units = 36, activation='relu'))
model.add(keras.layers.Dense(units = 18, activation='relu'))                                                ## Model Structure
model.add(keras.layers.Dense(units = 9, activation='relu'))
model.add(keras.layers.Dense(units = 2, activation='softmax'))

model.compile(optimizer=keras.optimizers.Adam(1e-5), loss = 'binary_crossentropy', metrics=['binary_accuracy'])

model.summary()

model.fit(X_train, Y_train, epochs=50, validation_data=(X_test, Y_test))                ## Training Model

X_val = test.to_numpy()[:, 1]
X_val_encoded = np.array([enc.n_gram_encode(i, n_gram) for i in X_val]).reshape(-1, 6**n_gram)        ##Encoding test dataset

predictions = model.predict(X_val_encoded)                                                             #Predicting values

predictions = np.array([0 if i[0] >= 0.5 else 1 for i in predictions])                                  # converting prediction values in the form of 0 and 1

sample = pd.read_csv(r"{}".format(input()))

X_val_encoded.shape

sample['Label'] = predictions.reshape(-1, 1)

sample.to_csv("/content/drive/MyDrive/Protein Classification/result_keras_balanced_prob_values11.csv", index=False)       ## Saving the result to csv

